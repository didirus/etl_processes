{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2061380a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BONUS: Schaalvergroting met PySpark\n",
    "\n",
    "**Wanneer PySpark gebruiken?** Data > 10GB, 100+ files/dag, of processing > 30 min\n",
    "\n",
    "\n",
    "**Key verschillen met pandas:**\n",
    "- **Lazy evaluation**: PySpark bouwt een execution plan en voert pas uit bij `.collect()`, `.show()`, of `.write()`\n",
    "- **Immutability**: DataFrames zijn immutable (elke transformatie = nieuwe DF)\n",
    "- **Partitioning**: Data wordt automatisch verdeeld over cluster nodes\n",
    "- **Fault tolerance**: Auto-recovery bij node failures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f61c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Als dit lokaal niet werkt, probeer het dan in Google Colab:\n",
    "# https://colab.research.google.com/\n",
    "!pip install pyspark==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialiseer Spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SensorPipeline\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Europe/Amsterdam\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark versie: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract - lees alle JSON files parallel\n",
    "# In plaats van Python loop, laat Spark dit parallel doen\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "# Als JSON files een array bevatten ([]) - gebruik dan multiLine=True\n",
    "sensor_df = spark.read.option(\"multiLine\", \"true\").json(\"sample_data/sensor_readings_*.json\")\n",
    "\n",
    "print(f\"Gelezen: {sensor_df.count()} records\")\n",
    "print(\"\\nSchema:\")\n",
    "sensor_df.printSchema()\n",
    "\n",
    "# Transformeer DIRECT na extract: timestamp van string naar timestamp type\n",
    "sensor_df = sensor_df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "sensor_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ad1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Lees metadata\n",
    "metadata_df = spark.read.csv(\"sample_data/sensor_metadata.csv\", header=True, inferSchema=True)\n",
    "metadata_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e96d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Transform - declaratief en parallel\n",
    "\n",
    "# Spark vs. Pandas:\n",
    "# - Spark = lazy evaluation (bouwt execution plan, voert pas uit bij .show()/.write())\n",
    "# - Pandas = eager evaluation (elke operatie wordt meteen uitgevoerd)\n",
    "# - Spark's Catalyst optimizer kan deze chain optimaliseren voor beste performance\n",
    "\n",
    "from pyspark.sql.functions import hour, dayofweek\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "transformed_df = sensor_df \\\n",
    "    .withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType())) \\\n",
    "    .join(metadata_df.select(\"sensor_id\", \"sensor_type\", \"manufacturer\", \"model\"),\n",
    "          on=\"sensor_id\", how=\"left\") \\\n",
    "    .withColumn(\"hour_of_day\", hour(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"is_working_hours\",\n",
    "                (col(\"hour_of_day\").between(8, 18)) & (col(\"day_of_week\").between(2, 6))) \\\n",
    "    .filter(\n",
    "        ((col(\"sensor_type\") == \"TEMP\") & col(\"value\").between(10, 35)) |\n",
    "        ((col(\"sensor_type\") == \"CO2\") & col(\"value\").between(300, 5000))\n",
    "    )\n",
    "\n",
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d19112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Aggregaties - veel sneller dan pandas op grote datasets\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "hourly_avg = transformed_df \\\n",
    "    .groupBy(\"sensor_id\", \"sensor_type\", date_format(\"timestamp\", \"yyyy-MM-dd HH:00:00\").alias(\"hour\")) \\\n",
    "    .agg({\"value\": \"avg\", \"battery_level\": \"min\"}) \\\n",
    "    .withColumnRenamed(\"avg(value)\", \"avg_value\") \\\n",
    "    .withColumnRenamed(\"min(battery_level)\", \"min_battery\") \\\n",
    "    .orderBy(\"hour\", \"sensor_id\")\n",
    "\n",
    "print(\"\\nUurlijks gemiddelde per sensor:\")\n",
    "hourly_avg.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3249a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Load - partitioned write (belangrijk voor query performance!)\n",
    "# Partitioning = data fysiek gescheiden op disk per sensor_type\n",
    "transformed_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"sensor_type\") \\\n",
    "    .parquet(\"warehouse_spark/\")\n",
    "\n",
    "print(\"\\nData geschreven naar warehouse_spark/ (gepartitioneerd per sensor_type)\")\n",
    "print(\"Directory structuur:\")\n",
    "# ls -R warehouse_spark/ zou tonen:\n",
    "# warehouse_spark/sensor_type=TEMP/part-00000.parquet\n",
    "# warehouse_spark/sensor_type=CO2/part-00000.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Performance vergelijking (simulatie met dupes data)\n",
    "# Maak 1000x copies voor simulatie\n",
    "large_df = sensor_df\n",
    "for i in range(10):  # 2^10 = 1024x data\n",
    "    large_df = large_df.union(sensor_df)\n",
    "\n",
    "print(f\"\\nSimulatie met {large_df.count()} records (1024x origineel)\")\n",
    "\n",
    "# Pandas zou hier out-of-memory gaan, PySpark blijft werken\n",
    "# omdat het niet alles in geheugen hoeft te laden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Spark UI - monitoring\n",
    "print(f\"\\nSpark UI beschikbaar op: http://localhost:4040\")\n",
    "print(\"Hier zie je:\")\n",
    "print(\"  - Execution plans (DAG)\")\n",
    "print(\"  - Job stages en tasks\")\n",
    "print(\"  - Data shuffle operaties\")\n",
    "print(\"  - Node resource usage\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
